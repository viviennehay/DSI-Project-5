{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Values Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define question <br>\n",
    "\n",
    "This project focuses on understanding the differences between attitudes to work around the world, and what drives those differences. Specifically:  <br> <br>\n",
    "a) What different values systems exist around the globe (focused on attitudes to work)? <br>\n",
    "b) What socio-economic factors influence a country's values system? <br>\n",
    "c) How well can we predict a country's values system, based on these factors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules & set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from data_dictionary import *\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "_random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('./world-values-survey-data/WVS_Longitudinal_1981_2014_stata_v2015_04_18.dta', convert_categoricals=False) # index_col='S025', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check shape and nulls; inspect header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can relabel the columns using the data dictionary found at  http://www.worldvaluessurvey.org/WVSDocumentationWVL.jsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_labels = []\n",
    "\n",
    "for i in df.columns:\n",
    "    new_column_labels.append(column_map_dictionary.get(i, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_labels2 = []\n",
    "\n",
    "for i in new_column_labels:\n",
    "    new_column_labels2.append(column_map_dictionary.get(i.lower(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns = new_column_labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the country is listed as a code (in 'Country/region' column). We want to add the name of the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn each country/ code combination into an item in a list, by splitting on new lines\n",
    "country_list = country_list.split('\\n')\n",
    "\n",
    "# Create a list of country codes and country names\n",
    "country_code_list = []\n",
    "country_name_list = []\n",
    "for country_pair in country_list:\n",
    "    country_code_list.append(country_pair.split(':')[0])\n",
    "    country_name_list.append(country_pair.split(':')[1])\n",
    "    \n",
    "# Turn the country code / name lookup into a dataframe, to allow a merge with the original dataframe\n",
    "country_dictionary = {'country_code': country_code_list, 'country_name': country_name_list}\n",
    "country_lookup = pd.DataFrame(country_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge country names into original dataframe\n",
    "df['Country/region'] = df['Country/region'].astype(int) # match dtypes\n",
    "country_lookup['country_code'] = country_lookup['country_code'].astype(int)\n",
    "df = pd.merge(df, country_lookup, left_on='Country/region', right_on='country_code', how = 'left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country_year'] = df['country_name'].astype(str) + '_' + df['Year survey'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, zoom in on the columns of interest:  What is important in a job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_qs = important_in_a_job\n",
    "baseline_qs = ['Wave', 'Year survey', 'Country/region', 'country_name', 'country_year']\n",
    "job_df = df[baseline_qs + job_qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nulls\n",
    "job_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count values greater than or equal to zero (from the data dictionary, negative numbers indicate missing data)\n",
    "(job_df[job_qs] >= 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing in on the columns with the fewest missing data points:\n",
    "refined_job_qs = ['Important in a job: good pay',\n",
    " 'Important in a job: not too much pressure',\n",
    " 'Important in a job: good job security',\n",
    " 'Important in a job: a respected job',\n",
    " 'Important in a job: good hours',\n",
    " 'Important in a job: an opportunity to use initiative',\n",
    " 'Important in a job: generous holidays',\n",
    " 'Important in a job: that you can achieve something',\n",
    " 'Important in a job: a responsible job',\n",
    " 'Important in a job: a job that is interesting',\n",
    " 'Important in a job: a job that meets oneÂ´s abilities']\n",
    "\n",
    "refined_job_df = df[baseline_qs + refined_job_qs].copy()\n",
    "\n",
    "(refined_job_df[refined_job_qs] >= 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many people partially answered the questions\n",
    "refined_job_df['number_of_jobs_qs_answered'] = (refined_job_df[refined_job_qs] >= 0).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_job_df['number_of_jobs_qs_answered'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, most people answered all the questions (144k) or none (189k). \n",
    "# A small proportion (~6k) responsed to some but not all of the questions\n",
    "# Dropping respondants who did not answer all questiosn will improve consistency and reduce bias, for a relatively small data loss\n",
    "# Consequently, we will focus only on those respondants who answered all questions\n",
    "mask = refined_job_df['number_of_jobs_qs_answered'] == 11\n",
    "refined_job_df = refined_job_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the data and look for correlations (clustering of input variables / multicolinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataframe\n",
    "refined_job_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at correlation matrix between different questions\n",
    "sns.heatmap(refined_job_df[refined_job_qs].corr(), annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Though correlations are all < 0.5, we can see correlated clusters of variables:\n",
    "- Achieving something, responsible job and opportunity to use initiative (correlations of ~0.4)\n",
    "- Good hours, generous holidays, not too much pressure (correlations of ~0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataframe\n",
    "refined_job_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "country_counts = refined_job_df.groupby('country_year').count()\n",
    "country_counts[refined_job_qs].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f'Each question was, on average, answered by {country_counts.iloc[:,0].mean():.0f} people in each wave, with a \\\n",
    "min of {country_counts.iloc[:,0].min():.0f} and a max of {country_counts.iloc[:,0].max():.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how responses evolve on a country-by-country level over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate\n",
    "country_averages = refined_job_df.groupby('country_year').mean()\n",
    "country_averages['country_year'] = country_averages.index\n",
    "country_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add country name column\n",
    "country_averages['country_name'] = [country_averages.index[i][0:country_averages.index[i].find('_')] for i in range(0, len(country_averages.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess data availability\n",
    "country_averages['country_name'].value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages['country_name'].value_counts().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of the 64 countries included in the datset, 8 have data from three or more waves; we will focus on these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sub-dfs for each of these countries\n",
    "longitudinal_countries = ['Mexico', 'Japan', 'Argentina', 'India', 'China', 'Chile', 'Turkey','Spain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, df in country_averages.groupby(country_averages.index):\n",
    "    if index == 'Mexico':\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution of values over time\n",
    "\n",
    "country_averages.index = country_averages['country_name']\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = int(len(longitudinal_countries)//n_cols)\n",
    "figure, ax = plt.subplots(nrows = n_rows, ncols=n_cols, figsize=(18,36))\n",
    "i = 0 # this is a counter to tell matplotlib which axis to plot on\n",
    "_cmap = plt.get_cmap('tab20')\n",
    "\n",
    "for index, dataframe in country_averages.groupby(country_averages.index):\n",
    "    if index in longitudinal_countries:\n",
    "        ax[i//n_cols][i%n_cols].set_title(f\"\\n {index} \\n\", fontsize = 15)\n",
    "        for j in range(0, len(refined_job_qs)): # this for loop is used to improve line colors on charts\n",
    "            ax[i//n_cols][i%n_cols].plot(dataframe['Year survey'], dataframe[refined_job_qs[j]], c=_cmap.colors[j])\n",
    "        ax[i//n_cols][i%n_cols].set_xlabel('Year', horizontalalignment = 'right')\n",
    "        ax[i//n_cols][i%n_cols].set_ylabel('Importance score')\n",
    "        if i == 0:\n",
    "            ax[i//n_cols][i%n_cols].legend(refined_job_qs, )\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing our data\n",
    "\n",
    "In some countries (e.g., Japan, Mexico) the variables tend to trend up and down together. This could be an artefact of how the survey was administered: the value for each cell is the percent of respondants who mentioned something as important. In some years, the interviews may have been longer or more comprehensive than others. To account for this fact, we can look at the number of times an attribute was mentioned as a % of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages['total_mentions'] = country_averages[refined_job_qs].sum(axis = 1)\n",
    "country_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in refined_job_qs:\n",
    "    country_averages[f'normalized {column}']=country_averages[column]/country_averages['total_mentions'] * 100 \n",
    "    # * 100 to get pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_refined_job_qs = [f'normalized {column}'for column in refined_job_qs]\n",
    "country_averages['normalized total_mentions'] = country_averages[normalized_refined_job_qs].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot evolution of values over time\n",
    "\n",
    "country_averages.index = country_averages['country_name']\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = int(len(longitudinal_countries)//n_cols)\n",
    "figure, ax = plt.subplots(nrows = n_rows, ncols=n_cols, figsize=(18,30))\n",
    "i = 0 # this is a counter to tell matplotlib which axis to plot on\n",
    "_cmap = plt.get_cmap('tab20')\n",
    "\n",
    "for index, dataframe in country_averages.groupby(country_averages.index):\n",
    "    if index in longitudinal_countries:\n",
    "        ax[i//n_cols][i%n_cols].set_title(f\"\\n {index} \\n\", fontsize = 15)\n",
    "        for j in range(0, len(normalized_refined_job_qs)): # this for loop is used to improve line colors on charts\n",
    "            ax[i//n_cols][i%n_cols].plot(dataframe['Year survey'], dataframe[normalized_refined_job_qs[j]], c=_cmap.colors[j])\n",
    "        ax[i//n_cols][i%n_cols].set_xlabel('Year', horizontalalignment = 'right')\n",
    "        # ax[i//n_cols][i%n_cols].set_xlim(xmin = 1980)\n",
    "        ax[i//n_cols][i%n_cols].set_ylabel('Percent of total mentions')\n",
    "        ax[i//n_cols][i%n_cols].legend(refined_job_qs, loc = 'lower left')\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering for clustering analysis\n",
    "\n",
    "As noted above, we can see that certain variables are correlated:\n",
    "- Achieving something, responsible job and opportunity to use initiative (correlations of ~0.4)\n",
    "- Good hours, generous holidays, not too much pressure (correlations of ~0.35)\n",
    "\n",
    "Using a methodology similar to PCA, we can create new features to reduce the dimensionality of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at correlation matrix between different questions\n",
    "sns.heatmap(country_averages[normalized_refined_job_qs].corr(), annot = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good hours, not too much pressure, holidays\n",
    "# Responsible, initiative, achieve\n",
    "# Security, pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_refined_job_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "\n",
    "# Achieving_responsible_initiative\n",
    "\n",
    "country_averages['normalized achieving_responsible_initiative'] = \\\n",
    "    country_averages['normalized Important in a job: that you can achieve something'] + \\\n",
    "    country_averages['normalized Important in a job: a responsible job'] + \\\n",
    "    country_averages['normalized Important in a job: an opportunity to use initiative']\n",
    "\n",
    "# Hours_holidays_pressure\n",
    "\n",
    "country_averages['normalized hours_holidays_pressure'] = \\\n",
    "    country_averages['normalized Important in a job: good hours'] + \\\n",
    "    country_averages['normalized Important in a job: generous holidays'] + \\\n",
    "    country_averages['normalized Important in a job: not too much pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages['normalized achieving_responsible_initiative'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages['normalized hours_holidays_pressure'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "country_averages[normalized_refined_job_qs].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, these two features account for ~45% of the variance. \"Good pay\" and \"good job security\" are also important, accounting for ~14% and ~12% of responses respectively. In addition, they are correlated (0.32) so we'll try engineering a new feature combining both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security_pay\n",
    "\n",
    "country_averages['normalized security_pay'] = \\\n",
    "    country_averages['normalized Important in a job: good job security'] + \\\n",
    "    country_averages['normalized Important in a job: good pay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages['normalized security_pay'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_job_attributes = ['normalized achieving_responsible_initiative', \n",
    "                             'normalized hours_holidays_pressure', \n",
    "                             'normalized security_pay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our three engineered features account for ~75% of responses, using only three features vs. the original eleven. While this is not as mathematically rigorous as PCA (in terms of maximizing variance captured), creating meaningful combinations of variables helps us maintain interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a plotter function\n",
    "\n",
    "def plotter(df, input_cols, \\\n",
    "            k_means_n_clusters = 3, agglom_n_clusters = 3, \\\n",
    "            dbscan_eps = 0.5, dbscan_min_samples = 5, n_rows = 1, n_cols = 3, _cmap = plt.get_cmap('tab20')):\n",
    "    '''function to visualize kmean, agglomerative and dbscan clustering\n",
    "    args: dataframe, input_cols --> to pull in from df, \n",
    "    kwargs: k_means_n_clusters, agglom_n_clusters, dbscan_eps, dbscan_min_samples, n_rows, n_cols --> rows/ cols of subplots, _cmap'''\n",
    "    # pull out columns to plot and scale\n",
    "    cols_to_plot = df[input_cols]\n",
    "    figure, ax = plt.subplots(nrows = n_rows, ncols=n_cols, figsize=(18,5))\n",
    "    # k means\n",
    "    kmeans = KMeans(n_clusters=k_means_n_clusters)\n",
    "    kmeans.fit(cols_to_plot)\n",
    "    df['kmeans_labels'] = kmeans.labels_\n",
    "    for index, sub_dataframe in df.groupby('kmeans_labels'):\n",
    "        ax[0].set_title(\"K means\")\n",
    "        ax[0].scatter(sub_dataframe[input_cols[0]], sub_dataframe[input_cols[1]], c=_cmap.colors[index], label = f\"Index {index}\")\n",
    "        ax[0].set_xlabel(input_cols[0])\n",
    "        ax[0].set_ylabel(input_cols[1])\n",
    "        ax[0].legend()\n",
    "    # agglom\n",
    "    agglom = AgglomerativeClustering(n_clusters=agglom_n_clusters)\n",
    "    agglom.fit(cols_to_plot)\n",
    "    df['agglom_labels'] = agglom.labels_\n",
    "    for index, sub_dataframe in df.groupby('agglom_labels'):\n",
    "        ax[1].set_title(\"Agglomerative clustering\")\n",
    "        ax[1].scatter(sub_dataframe[input_cols[0]], sub_dataframe[input_cols[1]], c=_cmap.colors[index], label = f\"Index {index}\")\n",
    "        ax[1].set_xlabel(input_cols[0])\n",
    "        ax[1].set_ylabel(input_cols[1])\n",
    "        ax[1].legend()\n",
    "    # dbscan\n",
    "    dbmodel = DBSCAN(eps = dbscan_eps, min_samples=dbscan_min_samples)\n",
    "    dbmodel.fit(cols_to_plot)\n",
    "    df['dbscan_labels'] = dbmodel.labels_\n",
    "    for index, sub_dataframe in df.groupby('dbscan_labels'):\n",
    "        ax[2].set_title(\"DBSCAN\")\n",
    "        ax[2].scatter(sub_dataframe[input_cols[0]], sub_dataframe[input_cols[1]], c=_cmap.colors[index], label = f\"Index {index}\")\n",
    "        ax[2].set_xlabel(input_cols[0])\n",
    "        ax[2].set_ylabel(input_cols[1])\n",
    "        ax[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot countries, using different clustering algorithms and variables\n",
    "plotter(df = country_averages, \n",
    "        input_cols = ['normalized achieving_responsible_initiative', 'normalized hours_holidays_pressure'],\n",
    "        k_means_n_clusters = 3, agglom_n_clusters = 3, dbscan_eps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(df = country_averages, \n",
    "        input_cols = ['normalized achieving_responsible_initiative', 'normalized security_pay'],\n",
    "        k_means_n_clusters = 3, agglom_n_clusters = 3, dbscan_eps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(df = country_averages, \n",
    "        input_cols = ['normalized hours_holidays_pressure', 'normalized security_pay'],\n",
    "        k_means_n_clusters = 3, agglom_n_clusters = 3, dbscan_eps = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top left chart (kmeans clustering with the two axes as achieving/ responsible/ initiative and hours/ holidays/ pressure accounts) provides the best visual separation. Proceeding with this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify our plotter to focus on k-means and add point labels\n",
    "\n",
    "def k_means_plotter(df, input_cols, k_means_n_clusters = 3, n_rows = 1, n_cols = 1, _cmap = plt.get_cmap('tab20')):\n",
    "    '''function to visualize kmean clustering\n",
    "    args: dataframe, input_cols --> to pull in from df, agglom_n_clusters, dbscan_eps, dbscan_min_samples\n",
    "    kwargs: k_means_n_clusters = 3, n_rows = 1, n_cols =1 --> rows/ cols of subplots, _cmap = plt.get_cmap('tab20')'''\n",
    "    # pull out columns to plot and scale\n",
    "    cols_to_plot = df[input_cols]\n",
    "    figure, ax = plt.subplots(nrows = n_rows, ncols=n_cols, figsize=(18,11))\n",
    "    # k means\n",
    "    kmeans = KMeans(n_clusters=k_means_n_clusters, random_state=_random_state)\n",
    "    kmeans.fit(cols_to_plot)\n",
    "    df['kmeans_labels'] = kmeans.labels_\n",
    "    for index, sub_dataframe in df.groupby('kmeans_labels'):\n",
    "        ax.set_title(\"\\n Country value systems\\n \", size = 18)\n",
    "        ax.scatter(sub_dataframe[input_cols[0]], sub_dataframe[input_cols[1]], c=_cmap.colors[index], label = f\"Index {index}\")\n",
    "        ax.set_xlabel('% of responses focused on achievement', size = 14)\n",
    "        ax.tick_params(labelsize = 14)\n",
    "        #ax.set_xlim(xmax = 33)\n",
    "        ax.set_ylabel('% of responses focused on lifestyle', size = 14)\n",
    "        for j, k in sub_dataframe['country_year'].iteritems():\n",
    "            x = sub_dataframe[input_cols[0]][j]+.1\n",
    "            y = sub_dataframe[input_cols[1]][j]+.05\n",
    "            label = k\n",
    "            ax.annotate(label, (x, y), rotation= 20, ha = 'left', va = 'bottom', size = 10)\n",
    "        ax.legend([\"Balanced\",\"Achievement\",\"Lifestyle\"], fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset index to unique values\n",
    "country_averages.index = country_averages['country_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_plotter(df = country_averages, \n",
    "        input_cols = ['normalized achieving_responsible_initiative', 'normalized hours_holidays_pressure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The clustering analysis identifies three groups:\n",
    "- **The lifestyle group**: Countries where people are less concerned with having a high-achieving job and moderately concerned about lifestyle (vacations, hours, pressure) - including Russia, Albania and Slovakia\n",
    "- **The achievement group**: Countries where people want a high-achieving job and care less about lifestyle - including Norway, Sweden and Australia\n",
    "- **The balanced group**: Countries that balance achievement with lifestyle - including Turkey, Morocco and South Korea\n",
    "\n",
    "#### Next, we will use multi-class logistic regression to see if we can predict a country's cluster based on economic variables.\n",
    "\n",
    "#### Two notes:\n",
    "- The boundaries between the groups are somewhat fuzzy (e.g., the U.S. in 1990 could also be assigned to the high-achievement group\n",
    "- Many countries (for example Japan and Turkey) are fairly stable in values over time. Building on this analysis, we could look at what drives changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in labels\n",
    "\n",
    "cluster_labels_df = pd.DataFrame([{'kmeans_labels': 1, 'kmeans_value_cluster' :'achievement'},\n",
    "                                  {'kmeans_labels': 2, 'kmeans_value_cluster' :'lifestyle'},\n",
    "                                  {'kmeans_labels': 0, 'kmeans_value_cluster' :'balanced'}])\n",
    "\n",
    "country_averages = pd.merge(country_averages, cluster_labels_df, on = 'kmeans_labels')\n",
    "country_averages.index = country_averages['country_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm value clusters correctly mapped by looking at one example from each cluster\n",
    "balanced_check = country_averages.loc['Uganda_2001','kmeans_value_cluster'] # balanced cluster\n",
    "achievement_check = country_averages.loc['Peru_1996','kmeans_value_cluster'] # achievement cluster\n",
    "lifestyle_check = country_averages.loc['Finland_1981','kmeans_value_cluster'] # lifestyle cluster\n",
    "\n",
    "balanced_check, achievement_check, lifestyle_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in indicator data - GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe (from https://data.worldbank.org/, accessed July 2018)\n",
    "gdp_df = pd.read_csv('./indicator_data/World_bank_GDP_per_cap_PPP_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check shape of dataframe\n",
    "gdp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check percent nulls\n",
    "print(f'{gdp_df.isnull().sum().sum()*100/(gdp_df.shape[0]*gdp_df.shape[1]):.0f}% null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The country_averages df has a two-level index (country_year). We need to stack our gdp dataframe to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdp_df.index = gdp_df['Country Name']\n",
    "gdp_df.drop('Country Name', axis = 1, inplace = True)\n",
    "\n",
    "gdp_df = pd.DataFrame(gdp_df.stack(), columns=[\"GDP per capita, PPP\"])\n",
    "\n",
    "gdp_df.reset_index(inplace=True)\n",
    "\n",
    "gdp_df.index = gdp_df['Country Name'] + \"_\" + gdp_df['level_1']\n",
    "\n",
    "gdp_df.drop(['Country Name', 'level_1'], axis = 1, inplace= True)\n",
    "\n",
    "gdp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_averages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our country_averages df has no nulls\n",
    "assert country_averages.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in GDP information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_gdp_df = pd.merge(country_averages, gdp_df, how = 'left', left_index = True, right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "job_and_gdp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assess introduction of null values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had introduced 30 nulls, out of 104 values. Refering to the null values using \"job_and_gdp_df.sort_values('GDP per capita, PPP', na_position='first').head(15)\" we can fix many of these by matching our country names to country names in the World Bank database (e.g., Dominican Republic vs. Dominican Rep.). Post-processing, 11 nulls remain. These are mostly due to timing, as the World Values Survey was administered in a few countries before 1990 whereas the World Bank PPP GDP figures begin in 1990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_gdp_df['Wave'].count(), job_and_gdp_df['GDP per capita, PPP'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the nulls from our dataframe going forwards. For easy reference, we will save the dataframe with nulls as job_and_gdp_df_original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_gdp_df_original = job_and_gdp_df\n",
    "job_and_gdp_df = job_and_gdp_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_and_gdp_df.shape, job_and_gdp_df_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in indicator data - GINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe (from https://data.worldbank.org/, accessed July 2018)\n",
    "gini_df = pd.read_csv('./indicator_data/gini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "gini_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute nulls\n",
    "gini_df.index = gini_df['Country Name']\n",
    "gini_df.drop('Country Name', axis = 1, inplace=True)\n",
    "gini_df.fillna(method = 'ffill', axis=1, inplace=True)         # first try a forward fill (i.e., base on the last available value)\n",
    "gini_df.fillna(method = 'backfill', axis=1, inplace= True)     # next try a backward fill (i.e., base on the next available value)\n",
    "gini_df.fillna(value = gini_df.mean(), inplace=True)           # where no data is available fill with the dataframe mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The country_averages df has a two-level index (country_year). We need to stack our gini dataframe to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gini_df = pd.DataFrame(gini_df.stack(), columns=[\"GINI coefficient\"])\n",
    "\n",
    "gini_df.reset_index(inplace=True)\n",
    "\n",
    "gini_df.index = gini_df['Country Name'] + \"_\" + gini_df['level_1']\n",
    "\n",
    "gini_df.drop(['Country Name', 'level_1'], axis = 1, inplace= True)\n",
    "\n",
    "gini_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our country_averages df has no nulls\n",
    "assert country_averages.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in GINI information\n",
    "\n",
    "job_and_indicator_df = pd.merge(job_and_gdp_df, gini_df, how = 'left', left_index = True, right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview new dataframe\n",
    "\n",
    "job_and_indicator_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assess introduction of null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_indicator_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We had introduced 22 nulls, out of 93 values. We can fix many of these by matching our country names to country names in the World Bank database (e.g., Dominican Republic vs. Dominican Rep.). \n",
    "- Post-processing, 5 nulls remained. These are due to a handful of countries that do not report GINI scores (New Zealand, Puerto Rico, Saudi Arabia, Singapore)\n",
    "- These remaining nulls were imputed with the average GINI coeff across all countries, leading to no additional nulls being inserted into our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in indicator data - primary school completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe (from https://data.worldbank.org/, accessed July 2018)\n",
    "primary_completion_df = pd.read_csv('./indicator_data/primary_completion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "primary_completion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute nulls\n",
    "primary_completion_df.index = primary_completion_df['Country Name']\n",
    "primary_completion_df.drop('Country Name', axis = 1, inplace=True)\n",
    "primary_completion_df.fillna(method = 'ffill', axis=1, inplace=True)         # first try a forward fill (i.e., base on the last available value)\n",
    "primary_completion_df.fillna(method = 'backfill', axis=1, inplace= True)     # next try a backward fill (i.e., base on the next available value)\n",
    "\n",
    "primary_completion_df.fillna(value = 100, inplace=True)           # three countries have no data available for any years:\n",
    "# the United States, Australia and Bosnia. Since primary education is compulsory and free in these countries,\n",
    "# we impute a value of close to 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_completion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The country_averages df has a two-level index (country_year). We need to stack our education dataframe to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primary_completion_df = pd.DataFrame(primary_completion_df.stack(), columns=[\"Primary completion rate\"])\n",
    "\n",
    "primary_completion_df.reset_index(inplace=True)\n",
    "\n",
    "primary_completion_df.index = primary_completion_df['Country Name'] + \"_\" + primary_completion_df['level_1']\n",
    "\n",
    "primary_completion_df.drop(['Country Name', 'level_1'], axis = 1, inplace= True)\n",
    "\n",
    "primary_completion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our country_averages df has no nulls\n",
    "assert job_and_indicator_df.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in GINI information\n",
    "\n",
    "job_and_indicator_df = pd.merge(job_and_indicator_df, primary_completion_df, how = 'left', left_index = True, right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview new dataframe\n",
    "\n",
    "job_and_indicator_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assess introduction of null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_indicator_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We had introduced 53 nulls, out of 93 values. We can fix many of these by matching our country names to country names in the World Bank database (e.g., Dominican Republic vs. Dominican Rep.). \n",
    "- Post-processing, 43 nulls remained\n",
    "- Using back and forward fill to impute missing values, we can reduce the number of nulls to 5\n",
    "- These are due to a handful of countries that do not report primary school completion (Australia, Bosnia, United States)\n",
    "- Primary education is mandatory and free for children in all these countries, so we can impute a completion rate of close to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in indicator data - democracy index\n",
    "Overall polity score from the Polity IV dataset, calculated by subtracting an autocracy score from a democracy score. It is a summary measure of a country's democratic and free nature. -10 is the lowest value, 10 the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe (from https://www.gapminder.org/data/ ('Democracy Index'), accessed July 2018)\n",
    "democracy_df = pd.read_csv(\"./indicator_data/Democracy_index_gapminder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "democracy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example country: UK\n",
    "# democracy_df.index = democracy_df['Democracy index']\n",
    "# UK = pd.DataFrame(democracy_df.loc['United Kingdom', :])\n",
    "# UK.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute nulls\n",
    "democracy_df.rename(columns = {'Democracy index':'Country Name'}, inplace=True)   # rename our country labels to match prior dataframes\n",
    "democracy_df.index = democracy_df['Country Name']\n",
    "democracy_df.drop('Country Name', axis = 1, inplace=True)\n",
    "democracy_df.fillna(method = 'ffill', axis=1, inplace=True)                     # first try a forward fill (i.e., base on the last available value)\n",
    "democracy_df.fillna(method = 'backfill', axis=1, inplace= True)                 # next try a backward fill (i.e., base on the next available value)\n",
    "\n",
    "democracy_df.fillna(value = democracy_df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The country_averages df has a two-level index (country_year). We need to stack our democracy dataframe to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "democracy_df = pd.DataFrame(democracy_df.stack(), columns=[\"Democracy index\"])\n",
    "\n",
    "democracy_df.reset_index(inplace=True)\n",
    "\n",
    "democracy_df.index = democracy_df['Country Name'] + \"_\" + democracy_df['level_1']\n",
    "\n",
    "democracy_df.drop(['Country Name', 'level_1'], axis = 1, inplace= True)\n",
    "\n",
    "democracy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our country_averages df has no nulls\n",
    "assert job_and_indicator_df.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in democracy information\n",
    "\n",
    "job_and_indicator_df = pd.merge(job_and_indicator_df, democracy_df, how = 'left', left_index = True, right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview new dataframe\n",
    "\n",
    "job_and_indicator_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assess introduction of null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_indicator_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_and_indicator_df.sort_values('Democracy index', na_position='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We had introduced 10 nulls, out of 93 values. We can fix many of these by matching our country names to country names in the World Bank database (e.g., Dominican Republic vs. Dominican Rep.). \n",
    "- Post-processing, 5 nulls remained\n",
    "- Using back and forward fill to impute missing values, we can reduce the number of nulls to 2\n",
    "- There is no score for Puerto Rico, but we can impute this with the mean for the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create baseline for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified three clusters of attitudes towards jobs - the lifestyle group, the achievement group and the balanced group. Our model will be a logistic regression model to assess whether we can predict which cluster a country is in at a given point in time, based on its GDP.\n",
    "\n",
    "The simplest model we could create is to predict the majority class each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identifying the majority class\n",
    "job_and_gdp_df['kmeans_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Baseline accuracy (i.e., the accuracy of a model that just predicts the majority class)\n",
    "print(f'Baseline accuracy = {max(job_and_gdp_df[\"kmeans_labels\"].value_counts())/ job_and_gdp_df.shape[0]*100:.0f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize features to allow for feature importance selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_features = ['GDP per capita, PPP', 'GINI coefficient', 'Primary completion rate', 'Democracy index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_indicator_columns = pd.DataFrame(ss.fit_transform(job_and_indicator_df[indicator_features]),\n",
    "                                       columns = ['Scaled GDP per capita, PPP', 'Scaled GINI coefficient', 'Scaled Primary completion rate', 'Scaled Democracy index'],\n",
    "                                       index = job_and_indicator_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_indicator_df = pd.merge(job_and_indicator_df, scaled_indicator_columns, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_and_indicator_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine test size\n",
    "job_and_indicator_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 93 rows of data. Using a 80/20% train test split gives us ~75 rows to train our model and ~18 to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a linear regression, we need our data to be ordinal categories (vs. disparate categories)\n",
    "# We can roughly order them by making 'lifestyle' -1, 'balanced' 0 and 'achievement' 1\n",
    "\n",
    "ordinal_mapping = pd.DataFrame([{'kmeans_value_cluster':'lifestyle','ordinal_kmeans_label': -1},\n",
    "                  {'kmeans_value_cluster':'balanced','ordinal_kmeans_label': 0},\n",
    "                  {'kmeans_value_cluster':'achievement','ordinal_kmeans_label': 1}])\n",
    "\n",
    "job_and_indicator_df = pd.merge(job_and_indicator_df, ordinal_mapping, on = 'kmeans_value_cluster')\n",
    "job_and_indicator_df.index = job_and_indicator_df['country_year']\n",
    "\n",
    "job_and_indicator_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [i for i in job_and_indicator_df.columns if i !='ordinal_kmeans_label']\n",
    "target = 'ordinal_kmeans_label'\n",
    "train_X, test_X, train_y, test_y = train_test_split(job_and_indicator_df[feature_list], job_and_indicator_df[target], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build linear regression model to predict attitudes towards jobs from development indicator values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_achievement_axis = LinearRegression()\n",
    "lr_achievement_axis.fit(train_X[indicator_features], train_X['normalized achieving_responsible_initiative'])\n",
    "lr_achievement_axis.score(test_X[indicator_features], test_X['normalized achieving_responsible_initiative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_lifestyle_axis = LinearRegression()\n",
    "lr_lifestyle_axis.fit(train_X [indicator_features], train_X ['normalized hours_holidays_pressure'])\n",
    "lr_lifestyle_axis.score(test_X[indicator_features], test_X ['normalized hours_holidays_pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_income_security_axis = LinearRegression()\n",
    "lr_income_security_axis.fit(train_X[indicator_features], train_X['normalized security_pay'])\n",
    "lr_income_security_axis.score(test_X[indicator_features], test_X['normalized security_pay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of folds for cross validation. We use CV = 10. While higher than a typical value of 3-5,  this will \n",
    "# allow us to use a higher portion of our dataset to train the model which is useful as we have a small dataset\n",
    "_cv = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in engineered_job_attributes:\n",
    "    cvs = cross_val_score(LinearRegression(),\n",
    "                                       train_X[indicator_features], \n",
    "                                       train_X[i], \n",
    "                                       cv = _cv)\n",
    "    print(f'\\n The mean and range of R2 values for **{i}** are {cvs.mean():.2f} and {cvs.min():.2f} - \\\n",
    "{cvs.max():.2f}. The values are {list(np.round(cvs, 2))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Overall, we see that our features are fairly good at predicting the achievement axis, but less successful at predicting the lifestyle axis (holiday/hours/pressure) and income security axis (security/pay). Our clustering analysis builds on both the achievement and lifestyle axes, so our logistic regression will be able to provide some prediction of cluster based on the achievement axis but will struggle on the lifestyle axis. One possible build is to add more features that better predict the lifestyle axis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update data: use normalized data to allow feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in engineered_job_attributes:\n",
    "    cvs = cross_val_score(LinearRegression(),\n",
    "                                       train_X[i].values.reshape(-1,1), \n",
    "                                       train_y, \n",
    "                                       cv = _cv)\n",
    "    print(f'\\n The mean and range of R2 values for **{i}** are {cvs.mean():.2f} and {cvs.min():.2f} - \\\n",
    "{cvs.max():.2f}. The values are {list(np.round(cvs, 2))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We want to understand which features are most important in predicting attitudes towards jobs. This section will focus coefficient interpretation the achivement axis, as our model is most effective for that axis. (Later, we'll map in more data to get a better prediction on the lifestyle axis. For now we'll ignore the income security axis, as that's not used in our clustering analysis)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_indicators = ['Scaled GDP per capita, PPP', 'Scaled GINI coefficient',\n",
    "       'Scaled Primary completion rate', 'Scaled Democracy index']\n",
    "cvs_achievement_axis_normalized = cross_val_score(LinearRegression(), \n",
    "                                                  train_X[scaled_indicators], train_y, cv = _cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvs_achievement_axis_normalized.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map in more features that could be used to predict the 'lifestyle' axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell subscriptions - https://data.worldbank.org/indicator/IT.CEL.SETS.P2?end=2016&start=1995&view=chart\n",
    "# life expectancy - https://data.worldbank.org/indicator/SP.DYN.LE00.FE.IN?view=chart\n",
    "# employment in ag - https://data.worldbank.org/indicator/SL.AGR.EMPL.MA.ZS?view=chart\n",
    "# employment in industry - https://data.worldbank.org/indicator/SL.IND.EMPL.MA.ZS?view=chart\n",
    "# employment in services - https://data.worldbank.org/indicator/SL.SRV.EMPL.MA.ZS?view=chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_subs_df = pd.read_csv(\"./indicator_data/cell_subscriptions.csv\")\n",
    "life_expectancy_female_df = pd.read_csv(\"./indicator_data/Life expectancy female.csv\")\n",
    "primary_employment_male_df = pd.read_csv(\"./indicator_data/Ag employment pct male.csv\")\n",
    "secondary_employment_male_df = pd.read_csv(\"./indicator_data/Industry employment pct male.csv\")\n",
    "tertiary_employment_male_df = pd.read_csv(\"./indicator_data/Services employment pct male.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute nulls\n",
    "\n",
    "new_indicator_dfs = [cell_subs_df, life_expectancy_female_df, \n",
    "                     primary_employment_male_df, secondary_employment_male_df, tertiary_employment_male_df]\n",
    "\n",
    "for i in range(0, len(new_indicator_dfs)):\n",
    "    new_indicator_dfs[i].index = new_indicator_dfs[i]['Country Name']\n",
    "    new_indicator_dfs[i].drop('Country Name', axis = 1, inplace=True)\n",
    "    new_indicator_dfs[i] = new_indicator_dfs[i].fillna(method = 'ffill', axis=1)             # first try a forward fill (i.e., base on the last available value)\n",
    "    new_indicator_dfs[i] = new_indicator_dfs[i].fillna(method = 'backfill', axis=1)          # next try a backward fill (i.e., base on the next available value)\n",
    "    new_indicator_dfs[i] = new_indicator_dfs[i].fillna(value = new_indicator_dfs[i].mean())  # finally impute with df mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
